@article{glickman1995glicko,
	title={The glicko system},
	author={Glickman, Mark E},
	journal={Boston University},
	year={1995}
}



@online{leela,
	author = {Gian-Carlo Pascutto, Gary Linscott},
	title = {Leela Chess Zero},
	year = 2018,
	url = {https://lczero.org/}
}

@online{stockfish,
	author = {Tord Romstad, Marco Costalba, Joona Kiiski, et al.},
	title = {Stockfish: A strong open source chess
	engine},
	year = 2018,
	url = {https://stockfishchess.org/}
}

@online{lichess,
	author = {Thibault Duplessis},
	title = {Lichess},
	year = 2018,
	url = {https://database.lichess.org}
}

@article {Silver1140,
	author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
	title = {A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
	volume = {362},
	number = {6419},
	pages = {1140--1144},
	year = {2018},
	doi = {10.1126/science.aar6404},
	publisher = {American Association for the Advancement of Science},
	abstract = {Computers can beat humans at increasingly complex games, including chess and Go. However, these programs are typically constructed for a particular game, exploiting its properties, such as the symmetries of the board on which it is played. Silver et al. developed a program called AlphaZero, which taught itself to play Go, chess, and shogi (a Japanese version of chess) (see the Editorial, and the Perspective by Campbell). AlphaZero managed to beat state-of-the-art programs specializing in these three games. The ability of AlphaZero to adapt to various game rules is a notable step toward achieving a general game-playing system.Science, this issue p. 1140; see also pp. 1087 and 1118The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.},
	issn = {0036-8075},
	URL = {http://science.sciencemag.org/content/362/6419/1140},
	eprint = {http://science.sciencemag.org/content/362/6419/1140.full.pdf},
	journal = {Science}
}


@article{kasparov2010chess,
	title={The chess master and the computer},
	author={Kasparov, Garry},
	journal={The New York Review of Books},
	volume={57},
	number={2},
	pages={16--19},
	year={2010}
}

@book{silver2012signal,
	title={The signal and the noise: why so many predictions fail--but some don't},
	author={Silver, Nate},
	year={2012},
	publisher={Penguin}
}

@article{silver2017mastering,
	title={Mastering chess and shogi by self-play with a general reinforcement learning algorithm},
	author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
	journal={arXiv preprint arXiv:1712.01815},
	year={2017}
}

@inproceedings{guid2009deriving,
	title={Deriving concepts and strategies from chess tablebases},
	author={Guid, Matej and Mo{\v{z}}ina, Martin and Sadikov, Aleksander and Bratko, Ivan},
	booktitle={Advances in Computer Games},
	pages={195--207},
	year={2009},
	organization={Springer}
}

@article{anderson2017assessing,
	title={Assessing human error against a benchmark of perfection},
	author={Anderson, Ashton and Kleinberg, Jon and Mullainathan, Sendhil},
	journal={ACM Transactions on Knowledge Discovery from Data (TKDD)},
	volume={11},
	number={4},
	pages={45},
	year={2017},
	publisher={ACM}
}

@inproceedings{david2016deepchess,
	title={Deepchess: End-to-end deep neural network for automatic learning in chess},
	author={David, Omid E and Netanyahu, Nathan S and Wolf, Lior},
	booktitle={International Conference on Artificial Neural Networks},
	pages={88--96},
	year={2016},
	organization={Springer}
}
@article{lai2015giraffe,
	title={Giraffe: Using deep reinforcement learning to play chess},
	author={Lai, Matthew},
	journal={arXiv preprint arXiv:1509.01549},
	year={2015}
}

@article{barnes2015limits,
	title={On the limits of engine analysis for cheating detection in chess},
	author={Barnes, David J and Hernandez-Castro, Julio},
	journal={Computers \& Security},
	volume={48},
	pages={58--73},
	year={2015},
	publisher={Elsevier}
}

@article{gullapalli1990stochastic,
	title={A stochastic reinforcement learning algorithm for learning real-valued functions},
	author={Gullapalli, Vijaykumar},
	journal={Neural networks},
	volume={3},
	number={6},
	pages={671--692},
	year={1990},
	publisher={Elsevier}
}

@inproceedings{tamar2012policy,
	title={Policy gradients with variance related risk criteria},
	author={Tamar, Aviv and Di Castro, Dotan and Mannor, Shie},
	booktitle={Proceedings of the twenty-ninth international conference on machine learning},
	pages={387--396},
	year={2012}
}

@article{lavalle2001randomized,
	title={Randomized kinodynamic planning},
	author={LaValle, Steven M and Kuffner Jr, James J},
	journal={The international journal of robotics research},
	volume={20},
	number={5},
	pages={378--400},
	year={2001},
	publisher={SAGE Publications}
}